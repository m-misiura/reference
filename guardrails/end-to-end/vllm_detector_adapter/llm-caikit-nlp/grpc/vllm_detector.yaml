apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-models-claim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 300Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: granite-detector-server
  labels:
    app: granite-detector-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: granite-detector-server
  template:
    metadata:
      labels:
        app: granite-detector-server
      annotations:
        prometheus.io/path: /metrics/
        prometheus.io/port: '3000'
        prometheus.io/scrape: 'true'
    spec:
      volumes:
        - name: model-volume
          persistentVolumeClaim:
            claimName: vllm-models-claim
        - name: shared-packages
          emptyDir: {}
      initContainers:
        - name: download-model
          image: quay.io/rgeada/llm_downloader:latest
          command:
            - bash
            - -c
            - |
              model="ibm-granite/granite-guardian-3.1-2b"
              echo "starting download"
              /tmp/venv/bin/huggingface-cli download $model --local-dir /mnt/models/llms/$(basename $model)
              echo "Done!"
          volumeMounts:
            - mountPath: "/mnt/models/"
              name: model-volume
        - name: vllm-guardrails-adapter
          image: quay.io/rh-ee-mmisiura/vllm-detector-adapter:3ad5b31
          command:
            - bash
            - -c
            - |
              echo "Creating directory structure..."
              mkdir -p /shared_packages/app
              
              echo "Copying packages..."
              cp -r /app/target_packages/* /shared_packages/app/
              
              echo "Verifying contents..."
              ls -la /shared_packages/app
          env:
            - name: MODEL_NAME
              value: ibm-granite/granite-guardian-3.1-2b
            - name: SHARED_PACKAGE_PATH
              value: /shared_packages/app
          volumeMounts:
            - name: shared-packages
              mountPath: /shared_packages
      containers:
        - name: vllm-server
          image: quay.io/opendatahub/vllm:fast-ibm-a58bf32
          resources:
            limits:
              cpu: '1'
              memory: 12Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '1'
              memory: 12Gi
              nvidia.com/gpu: '1'
          env:
            - name: MODEL_NAME
              value: ibm-granite/granite-guardian-3.1-2b
            - name: PORT
              value: "3000"
            - name: PYTHONPATH
              value: /shared_packages/app
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: HF_HOME
              value: /mnt/models/llms
          command:
            - bash
            - -c
            - |
              echo "Python version:"
              python3 --version
              echo "Current PYTHONPATH: $PYTHONPATH"
              echo "Contents of shared packages directory:"
              ls -la /shared_packages/app
              echo "Starting server..."
              python3 -m vllm_detector_adapter.api_server \
                --model ${MODEL_NAME} \
                --model-type granite_guardian \
                --dtype half \
                --max-model-len 8192
          ports:
            - name: http
              containerPort: 3000
              protocol: TCP
          volumeMounts:
            - name: model-volume
              mountPath: /mnt/models
            - name: shared-packages
              mountPath: /shared_packages
---
apiVersion: v1
kind: Service
metadata:
  name: granite-detector-server
  labels:
    app: granite-detector-server
spec:
  ports:
    - name: http
      port: 3000
      protocol: TCP
      targetPort: 8000
  selector:
    app: granite-detector-server
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: granite-detector-route
  labels:
    app: granite-detector-server
spec:
  to:
    kind: Service
    name: granite-detector-server
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Allow
  wildcardPolicy: None